{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c76792f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c733bdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in dataset\n",
    "train_file_path = 'THA2train.xlsx'\n",
    "validate_file_path = 'THA2validate.xlsx'\n",
    "train_dataset = pd.read_excel(train_file_path)\n",
    "validate_dataset = pd.read_excel(validate_file_path)\n",
    "\n",
    "X_train = train_dataset[['X_0', 'X_1']].values\n",
    "y_train = train_dataset['y'].values.reshape(-1, 1)\n",
    "X_val = validate_dataset[['X_0', 'X_1']].values\n",
    "y_val = validate_dataset['y'].values.reshape(-1, 1)\n",
    "\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_val = sc.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b04da2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Activation functions\n",
    "def sigmoid( x): # sigmoid function used at the hidden layer and output layer\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "def tanh(x):     # hyperbolic tangent\n",
    "    return np.tanh(x)\n",
    "def relu(x):     #RELU\n",
    "    return np.maximum(0, x)\n",
    "def leaky_relu(x, alpha=0.01):   #Leaky RELU\n",
    "    return np.where(x > 0, x, alpha * x)\n",
    "def elu(x, alpha=1.0):    #exponential LU\n",
    "    return np.where(x > 0, x, alpha * (np.exp(x) - 1))\n",
    "\n",
    "#Derivative of activation functions\n",
    "def sigmoid_derivative( x): # sigmoid derivative used for backpropgation\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "def tanh_derivative(x):\n",
    "    return 1 - np.tanh(x)**2\n",
    "def relu_derivative(x):\n",
    "    return np.where(x > 0, 1, 0)\n",
    "def leaky_relu_derivative(x, alpha=0.01):\n",
    "    return np.where(x > 0, 1, alpha)\n",
    "def elu_derivative(x, alpha=1.0):\n",
    "    return np.where(x > 0, 1, alpha * np.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "90fcaac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "class MLP:\n",
    "    def __init__(self, input_size = 2, hidden1_size = 10, hidden2_size = 10, output_size = 2):\n",
    "        #initialize parameters\n",
    "        self.input_size = input_size\n",
    "        self.hidden1_size = hidden1_size\n",
    "        self.hidden2_size = hidden2_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "#         # initialize weights randomly\n",
    "#         self.weights1 = np.random.randn(self.input_size, self.hidden1_size)\n",
    "#         self.weights2 = np.random.randn(self.hidden1_size, self.hidden2_size)\n",
    "#         self.weights3 = np.random.randn(self.hidden2_size, self.output_size)\n",
    "        \n",
    "#         #initialize weights to zero\n",
    "#         self.weights1 = np.zeros((self.input_size, self.hidden1_size))\n",
    "#         self.weights2 = np.zeros((self.hidden1_size, self.hidden2_size))\n",
    "#         self.weights3 = np.zeros((self.hidden2_size, self.output_size))\n",
    "\n",
    "#         # initialize biases to 0\n",
    "#         self.bias1 = np.zeros((1, self.hidden1_size))\n",
    "#         self.bias2 = np.zeros((1, self.hidden2_size))\n",
    "#         self.bias3 = np.zeros((1, self.output_size))\n",
    "\n",
    "    def forwardPropogation(self, X):\n",
    "        #Activation of hidden layer 1\n",
    "        hidden_layer1 = relu(X.dot(self.weights1) + self.bias1)\n",
    "        #Activation of hidden layer 2\n",
    "        hidden_layer2 = relu(hidden_layer1.dot(self.weights2) + self.bias2)\n",
    "        #Activation of output layer\n",
    "        output_layer = sigmoid(hidden_layer2.dot(self.weights3) + self.bias3)\n",
    "        return hidden_layer1, hidden_layer2, output_layer\n",
    "        \n",
    "    def backwardPropogation(self,X,y, activation1, activation2, activation3):\n",
    "        #derivative weights and biases for output layer\n",
    "        error = activation3 - y\n",
    "        d_weights3 = activation2.T.dot(error * sigmoid_derivative(activation3))\n",
    "        d_bias3 = np.sum(error * sigmoid_derivative(activation3), axis=0, keepdims=True)\n",
    "\n",
    "        #derivative weights and biases for hidden layer 2\n",
    "        error_hidden2 = error.dot(self.weights3.T) * relu_derivative(activation2)\n",
    "        d_weights2 = activation1.T.dot(error_hidden2)\n",
    "        d_bias2 = np.sum(error_hidden2, axis=0, keepdims=True)\n",
    "\n",
    "        #derivative weights and biases for hidden layer 1\n",
    "        error_hidden1 = error_hidden2.dot(self.weights2.T) * relu_derivative(activation1)\n",
    "        d_weights1 = X.T.dot(error_hidden1)\n",
    "        d_bias1 = np.sum(error_hidden1, axis=0, keepdims=True)\n",
    "        return d_weights1, d_weights2, d_weights3, d_bias1, d_bias2, d_bias3\n",
    "    \n",
    "    def updateParams(self, d_weights1, d_weights2, d_weights3, d_bias1, d_bias2, d_bias3, learning_rate):\n",
    "        self.weights3 -= learning_rate * d_weights3\n",
    "        self.bias3 -= learning_rate * d_bias3\n",
    "        self.weights2 -= learning_rate * d_weights2\n",
    "        self.bias2 -= learning_rate * d_bias2\n",
    "        self.weights1 -= learning_rate * d_weights1\n",
    "        self.bias1 -= learning_rate * d_bias1\n",
    "        \n",
    "    #Binary cross entropy\n",
    "    def bce(self, y_true, y_pred):\n",
    "        return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "    #Train the model\n",
    "    def train(self, X, y, epochs=100, learning_rate =0.01):\n",
    "        train_loss_history = []\n",
    "        output_result = []\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            # forward propogation\n",
    "            hidden_layer1, hidden_layer2, output_layer = self.forwardPropogation(X)\n",
    "            output_result.append(output_layer)\n",
    "\n",
    "            # Backpropagation\n",
    "            d_weights1, d_weights2, d_weights3, d_bias1, d_bias2, d_bias3 = self.backwardPropogation(X,y, hidden_layer1, hidden_layer2, output_layer)\n",
    "            \n",
    "\n",
    "            # Update weights and biases\n",
    "            self.updateParams(d_weights1, d_weights2, d_weights3, d_bias1, d_bias2, d_bias3, learning_rate)\n",
    "\n",
    "            #Training loss\n",
    "            train_loss = self.bce(y_train, output_layer)\n",
    "            train_loss_history.append(train_loss)\n",
    "            \n",
    "            y_train_pred = np.round(output_layer)\n",
    "            accuracy = np.mean(y_train_pred == y)\n",
    "            \n",
    "            if epoch%(epochs/10) == 0:\n",
    "                print(f\"Epoch {epoch+1} Error {train_loss} Accuracy {accuracy:.2f}\")\n",
    "                #print(\"Epoch: \"+str(epoch+1) + \" Error: \"+str(train_loss) + \" Accuracy: \"+str(accuracy))\n",
    "                \n",
    "                     \n",
    "        print('Training Complete')\n",
    "        print('----------------------------------------------------------------------------')\n",
    "        return train_loss_history\n",
    "\n",
    "    #Test the model\n",
    "    def test(self, X_val, y_val):\n",
    "        output_result = []\n",
    "        \n",
    "        hidden_layer1, hidden_layer2, output_layer = self.forwardPropogation(X_val)\n",
    "\n",
    "        # Compute validation loss\n",
    "        val_loss = self.bce(y_val, output_layer)\n",
    "        #val_loss_history.append(val_loss)\n",
    "            \n",
    "        y_val_pred = np.round(output_layer)\n",
    "        accuracy = np.mean(y_val_pred == y_val)\n",
    "        \n",
    "        output_result.append(y_val_pred)\n",
    "        \n",
    "        # Confusion matrix\n",
    "        confusion_matrix = np.zeros((2, 2))\n",
    "        confusion_matrix[0, 0] = np.sum((y_val == 0) & (y_val_pred == 0))\n",
    "        confusion_matrix[0, 1] = np.sum((y_val == 0) & (y_val_pred == 1))\n",
    "        confusion_matrix[1, 0] = np.sum((y_val == 1) & (y_val_pred == 0))\n",
    "        confusion_matrix[1, 1] = np.sum((y_val == 1) & (y_val_pred == 1))\n",
    "        \n",
    "        return accuracy, val_loss, confusion_matrix\n",
    "        \n",
    "\n",
    "#         # Check for early stopping based on validation loss\n",
    "#         if len(val_loss_history) > 100 and val_loss_history[-1] > np.mean(val_loss_history[-10:]):\n",
    "#             print(\"Stopping early as validation loss is not improving.\")\n",
    "#             break\n",
    "    \n",
    "    def model(self, X, y, X_val, y_val, epochs=100, learning_rate =0.01, retrain = 10):\n",
    "        sigma = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "        accuracy_hist = []\n",
    "        learning_rates = [0.00001, 0.00001, 0.00001, 0.0001, 0.0001, 0.0001, 0.001, 0.001, 0.001, 0.001]\n",
    "        for j in range(retrain):\n",
    "            \n",
    "            #Initialize weights\n",
    "            # initialize weights randomly\n",
    "            self.weights1 = np.random.normal(0, sigma[j], size=(self.input_size, self.hidden1_size))\n",
    "            self.weights2 = np.random.normal(0, sigma[j], size=(self.hidden1_size, self.hidden2_size))\n",
    "            self.weights3 = np.random.normal(0, sigma[j], size=(self.hidden2_size, self.output_size))\n",
    "\n",
    "            # initialize biases to 0\n",
    "            self.bias1 = np.zeros((1, self.hidden1_size))\n",
    "            self.bias2 = np.zeros((1, self.hidden2_size))\n",
    "            self.bias3 = np.zeros((1, self.output_size))\n",
    "\n",
    "            train_loss_history = []\n",
    "            val_loss_history = []\n",
    "            output_result = []\n",
    "            batch_size = 32\n",
    "\n",
    "\n",
    "\n",
    "            for epoch in range(epochs):\n",
    "                for i in range(0, len(X), batch_size):\n",
    "                    #Batch\n",
    "                    X_train = X[i : i + batch_size]\n",
    "                    y_train = y[i : i + batch_size]\n",
    "\n",
    "                    # forward propogation\n",
    "                    hidden_layer1, hidden_layer2, output_layer = self.forwardPropogation(X_train)\n",
    "                    output_result.append(output_layer)\n",
    "\n",
    "                    # Backpropagation\n",
    "                    d_weights1, d_weights2, d_weights3, d_bias1, d_bias2, d_bias3 = self.backwardPropogation(X_train,y_train, hidden_layer1, hidden_layer2, output_layer)\n",
    "\n",
    "\n",
    "                    # Update weights and biases\n",
    "                    self.updateParams(d_weights1, d_weights2, d_weights3, d_bias1, d_bias2, d_bias3, learning_rates[j])\n",
    "\n",
    "                    #Training loss\n",
    "                    train_loss = self.bce(y_train, output_layer)\n",
    "\n",
    "                train_loss_history.append(train_loss)\n",
    "\n",
    "                y_train_pred = np.round(output_layer)\n",
    "                train_accuracy = np.mean(y_train_pred == y_train)\n",
    "\n",
    "                #Test the data\n",
    "                val_accuracy, val_loss, confusion_matrix = self.test(X_val, y_val)\n",
    "                val_loss_history.append(val_loss)\n",
    "\n",
    "                #print(f\"Final accuracy of the model at training {j} is: {val_accuracy:.2f}\")\n",
    "                \n",
    "            print(f\"Trainin {j} Training loss {train_loss} Train Accuracy {train_accuracy:.2f} Validation loss {val_loss} Validation Accuracy {val_accuracy:2f}\")\n",
    "            \n",
    "            accuracy_hist.append(val_accuracy)\n",
    "        \n",
    "        print(accuracy_hist)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4dfb5246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainin 0 Training loss 0.6931472696843609 Train Accuracy 0.46 Validation loss 0.6931471805599454 Validation Accuracy 0.500000\n",
      "Trainin 1 Training loss 0.6933942700632534 Train Accuracy 0.50 Validation loss 0.693039409954994 Validation Accuracy 0.560976\n",
      "Trainin 2 Training loss 0.6896716550299912 Train Accuracy 0.56 Validation loss 0.6907491669209301 Validation Accuracy 0.554878\n",
      "Trainin 3 Training loss 0.19784695052483084 Train Accuracy 0.96 Validation loss 0.1729270838972068 Validation Accuracy 0.975610\n",
      "Trainin 4 Training loss 0.3558473368998823 Train Accuracy 0.87 Validation loss 0.3851025804401731 Validation Accuracy 0.878049\n",
      "Trainin 5 Training loss 0.2130420336541955 Train Accuracy 0.96 Validation loss 0.18044488832463154 Validation Accuracy 0.975610\n",
      "Trainin 6 Training loss 0.1461818971817463 Train Accuracy 0.96 Validation loss 0.15452361520821797 Validation Accuracy 0.975610\n",
      "Trainin 7 Training loss 0.1378634745131123 Train Accuracy 0.96 Validation loss 0.12482693376753738 Validation Accuracy 0.975610\n",
      "Trainin 8 Training loss 0.14604058560206729 Train Accuracy 0.96 Validation loss 0.15388012759411296 Validation Accuracy 0.975610\n",
      "Trainin 9 Training loss 0.1510915685500333 Train Accuracy 0.96 Validation loss 0.12772665058575586 Validation Accuracy 0.975610\n",
      "[0.5, 0.5609756097560976, 0.5548780487804879, 0.975609756097561, 0.8780487804878049, 0.975609756097561, 0.975609756097561, 0.975609756097561, 0.975609756097561, 0.975609756097561]\n"
     ]
    }
   ],
   "source": [
    "mlp1 = MLP()\n",
    "mlp1.model(X_train, y_train, X_val, y_val, epochs=500, learning_rate =0.001, retrain= 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
